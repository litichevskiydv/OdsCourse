{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Open Machine Learning Course\n",
    "<center>\n",
    "Author: Yury Kashnitsky, Data Scientist at Mail.Ru Group\n",
    "\n",
    "This material is subject to the terms and conditions of the license [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Free use is permitted for any non-comercial purpose with an obligatory indication of the names of the authors and of the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Assignment #6. Part 1\n",
    "### <center> Beating benchmarks in \"Catch Me If You Can: Intruder Detection through Webpage Session Tracking\"\n",
    "    \n",
    "[Competition](https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2). The task is to beat \"Assignment 6 baseline\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, time\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = ('../../data/user_identification')\n",
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate target feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df, scaler=None, vectorizer=None):\n",
    "    sites_df = df[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int').values\n",
    "    \n",
    "    sessions = list(map(lambda sites_ids: ' '.join(map(lambda site_id: str(site_id), filter(lambda site_id: site_id != 0, sites_ids))), sites_df))\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=100000, sublinear_tf=True)\n",
    "        vectorizer.fit(sessions)\n",
    "    tfidf_features = vectorizer.transform(sessions)\n",
    "    \n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(sites_df)\n",
    "    sites_df = scaler.transform(sites_df)\n",
    "    \n",
    "    df['session_start'] = df['time1'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "    df['year'] = df['session_start'].apply(lambda x: int(x.year))\n",
    "    df['month'] = df['session_start'].apply(lambda x: int(x.month))\n",
    "    df['day'] = df['session_start'].apply(lambda x: int(x.day))\n",
    "    df['dow'] = df['session_start'].apply(lambda x: int(x.weekday()))\n",
    "    df['is_weekend'] = df['dow'].apply(lambda x: 1 if x in (5, 6) else 0)\n",
    "    df['tod'] = df['session_start'].apply(lambda x: int(x.hour))\n",
    "    \n",
    "    df['is_night'] = df['tod'].apply(lambda x: 1 if 22 <= x <= 24 or 0 <= x < 8 else 0)\n",
    "    df['is_morning'] = df['tod'].apply(lambda x: 1 if 8 <= x < 10 else 0)\n",
    "    df['is_before_dinner'] = df['tod'].apply(lambda x: 1 if 10 <= x < 13 else 0)\n",
    "    df['is_dinner'] = df['tod'].apply(lambda x: 1 if 13 <= x < 15 else 0)\n",
    "    df['is_after_dinner'] = df['tod'].apply(lambda x: 1 if 15 <= x < 19 else 0)\n",
    "    df['is_evening'] = df['tod'].apply(lambda x: 1 if 19 <= x < 22 else 0)\n",
    "    df['is_alice_dow'] = df['dow'].apply(lambda x: 1 if x == 0 or x == 1 or x == 3 or x == 4 else 0)\n",
    "    df['is_alice_time'] = df['session_start'].apply(lambda x: 1 if 12 <= x.hour <= 13 or time(hour=15, minute=50) <= x.time() <= time(hour=18, minute=20) else 0)\n",
    "    \n",
    "    time_bool_features = df[['is_weekend', 'is_night', 'is_before_dinner', 'is_dinner', 'is_after_dinner', 'is_evening', 'is_alice_dow', 'is_alice_time']]\n",
    "    time_categorical_features = OneHotEncoder(n_values=[7, 24]).fit_transform(df[['dow', 'tod']])\n",
    "    \n",
    "    features = hstack([tfidf_features, time_bool_features, time_categorical_features]).tocsr()\n",
    "    return (scaler, vectorizer, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add features based on the session start time: hour, whether it's morning, day or night and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 s, sys: 250 ms, total: 36.3 s\n",
      "Wall time: 36.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(scaler, vectorizer, X) =  prepare_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale this features and combine then with Tf-Idf based on sites (you'll need `scipy.sparse.hstack`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_share = int(.7 * X.shape[0])\n",
    "X_train, y_train = X[:train_share, :], y[:train_share]\n",
    "X_valid, y_valid = X[train_share:, :], y[train_share:]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform cross-validation with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 0.3484, best score: 0.9626800081130419\n",
      "CPU times: user 4.93 s, sys: 140 ms, total: 5.07 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit_searcher = LogisticRegressionCV(Cs=[0.3484], cv=skf, scoring='accuracy', class_weight='balanced', random_state=17, n_jobs=-1)\n",
    "logit_searcher.fit(X_train, y_train)\n",
    "\n",
    "logit_scores = np.mean(list(map(lambda x: x[1], logit_searcher.scores_.items()))[0], axis=0)\n",
    "best_score_index = np.argmax(logit_scores)\n",
    "print('Best C: {0}, best score: {1}'.format(logit_searcher.Cs_[best_score_index], logit_scores[best_score_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9627969343622238\n",
      "0.055908713140284706\n",
      "0.9852330819393322\n",
      "CPU times: user 50 ms, sys: 0 ns, total: 50 ms\n",
      "Wall time: 52.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit_valid_pred_proba = logit_searcher.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "print(accuracy_score(y_valid, logit_searcher.predict(X_valid)))\n",
    "print(np.std(list(map(lambda x: x[1], logit_searcher.scores_.items()))[0], axis=0)[best_score_index]/logit_scores[best_score_index]*100)\n",
    "print(roc_auc_score(y_valid, logit_valid_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction for the test set and form a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 20 ms, total: 11.8 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit = LogisticRegression(C=logit_searcher.Cs_[best_score_index], class_weight='balanced', random_state=17, n_jobs=-1)\n",
    "logit.fit(X, y)\n",
    "\n",
    "(scaler, vectorizer, X_test) =  prepare_features(test_df, scaler, vectorizer)\n",
    "logit_test_pred_proba = logit.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(logit_test_pred_proba, \"assignment6_alice_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
